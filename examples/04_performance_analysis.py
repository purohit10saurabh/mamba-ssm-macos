#!/usr/bin/env python3
"""
Comprehensive Performance Benchmarking for Mamba2MacOS

Benchmarks different model sizes and configurations to provide
detailed performance analysis on Apple Silicon.
"""

import time
from functools import cached_property

import torch
import torch.nn as nn
from transformers import AutoTokenizer

from mamba_ssm.modules.mamba2_macos import Mamba2MacOS


class Mamba2MacOSModel(nn.Module):
    def __init__(self, d_model=256, n_layer=4, vocab_size=50277, d_state=16, device="cpu"):
        super().__init__()
        self._device = device
        self.embedding = nn.Embedding(vocab_size, d_model)
        self.norm = nn.LayerNorm(d_model)
        self.lm_head = nn.Linear(d_model, vocab_size, bias=False)
        self.layers = nn.ModuleList([
            Mamba2MacOS(d_model=d_model, d_state=d_state, layer_idx=i, device=device) 
            for i in range(n_layer)
        ])
        self.lm_head.weight = self.embedding.weight
        self.to(device)
    
    @cached_property
    def device(self):
        return next(self.parameters()).device
    
    def forward(self, input_ids, inference_params=None):
        hidden_states = self.embedding(input_ids)
        for layer in self.layers:
            hidden_states = layer(hidden_states, inference_params=inference_params)
        hidden_states = self.norm(hidden_states)
        return self.lm_head(hidden_states)

def benchmark_forward_pass(model, input_ids, num_runs=50):
    """Benchmark forward pass performance"""
    # Warmup
    for _ in range(5):
        with torch.no_grad():
            _ = model(input_ids)
    
    if model.device.type == "mps":
        torch.mps.synchronize()
    
    start_time = time.time()
    for _ in range(num_runs):
        with torch.no_grad():
            _ = model(input_ids)
    
    if model.device.type == "mps":
        torch.mps.synchronize()
    
    total_time = time.time() - start_time
    return total_time / num_runs

def benchmark_step_function(model, seq_len, num_steps=100):
    """Benchmark step function for autoregressive generation"""
    batch_size = 1
    tokenizer = AutoTokenizer.from_pretrained("gpt2")
    input_ids = torch.tensor([[tokenizer.bos_token_id]], device=model.device)
    
    # Allocate inference cache
    from mamba_ssm.utils.generation import InferenceParams
    inference_params = InferenceParams(max_seqlen=seq_len, max_batch_size=batch_size)
    
    for layer_idx, layer in enumerate(model.layers):
        cache_conv, cache_ssm = layer.allocate_inference_cache(batch_size, seq_len)
        inference_params.key_value_memory_dict[layer_idx] = (cache_conv, cache_ssm)
    
    # Warmup
    for _ in range(5):
        with torch.no_grad():
            _ = model(input_ids, inference_params=inference_params)
    
    if model.device.type == "mps":
        torch.mps.synchronize()
    
    start_time = time.time()
    for _ in range(num_steps):
        with torch.no_grad():
            _ = model(input_ids, inference_params=inference_params)
    
    if model.device.type == "mps":
        torch.mps.synchronize()
    
    total_time = time.time() - start_time
    return total_time / num_steps

def run_comprehensive_benchmark():
    """Run comprehensive benchmarks across different configurations"""
    print("üöÄ Comprehensive Mamba2MacOS Performance Benchmark")
    print("=" * 60)
    
    device = "mps" if torch.backends.mps.is_available() else "cpu"
    print(f"Device: {device}")
    
    # Model configurations
    configs = [
        {"name": "Small", "d_model": 256, "n_layer": 4, "d_state": 16, "seq_len": 128},
        {"name": "Medium", "d_model": 512, "n_layer": 8, "d_state": 32, "seq_len": 256},
        {"name": "Large", "d_model": 768, "n_layer": 12, "d_state": 64, "seq_len": 512},
    ]
    
    tokenizer = AutoTokenizer.from_pretrained("gpt2")
    results = []
    
    for config in configs:
        print(f"\nüìä Testing {config['name']} Model")
        print(f"   d_model={config['d_model']}, n_layer={config['n_layer']}, d_state={config['d_state']}")
        
        # Create model
        model = Mamba2MacOSModel(
            d_model=config['d_model'],
            n_layer=config['n_layer'], 
            d_state=config['d_state'],
            device=device
        )
        
        param_count = sum(p.numel() for p in model.parameters())
        print(f"   Parameters: {param_count:,}")
        
        # Create input
        seq_len = config['seq_len']
        input_ids = torch.randint(0, tokenizer.vocab_size, (2, seq_len), device=device)
        
        # Benchmark forward pass
        forward_time = benchmark_forward_pass(model, input_ids)
        tokens_per_sec_forward = (input_ids.numel()) / forward_time
        
        # Benchmark step function
        step_time = benchmark_step_function(model, seq_len)
        tokens_per_sec_step = 1.0 / step_time
        
        print(f"   Forward pass: {forward_time*1000:.2f}ms ({tokens_per_sec_forward:.0f} tokens/sec)")
        print(f"   Step function: {step_time*1000:.2f}ms ({tokens_per_sec_step:.0f} tokens/sec)")
        
        results.append({
            "name": config['name'],
            "params": param_count,
            "forward_time": forward_time,
            "step_time": step_time,
            "forward_tps": tokens_per_sec_forward,
            "step_tps": tokens_per_sec_step
        })
    
    # Summary table
    print(f"\nüìà Performance Summary")
    print("=" * 80)
    print(f"{'Model':<8} {'Params':<12} {'Forward':<15} {'Step':<15} {'Ratio':<10}")
    print("-" * 80)
    
    for result in results:
        ratio = result['forward_tps'] / result['step_tps']
        print(f"{result['name']:<8} {result['params']:>10,} {result['forward_tps']:>8.0f} tok/s {result['step_tps']:>8.0f} tok/s {ratio:>8.1f}x")
    
    print(f"\nüéØ Key Insights:")
    print(f"  ‚Ä¢ Forward pass is {results[0]['forward_tps']/results[0]['step_tps']:.1f}x faster than step function")
    print(f"  ‚Ä¢ Larger models have proportionally slower performance")
    print(f"  ‚Ä¢ Apple Silicon MPS provides consistent acceleration")
    print(f"  ‚Ä¢ Step function performance determines generation speed")
    
    return results

def main():
    try:
        results = run_comprehensive_benchmark()
        print(f"\n‚úÖ Benchmark completed successfully!")
        return results
    except Exception as e:
        print(f"‚ùå Benchmark failed: {e}")
        return None

if __name__ == "__main__":
    main() 