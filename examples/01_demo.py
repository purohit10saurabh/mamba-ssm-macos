#!/usr/bin/env python3
"""
Mamba Demo - Production Model Text Generation

This example demonstrates text generation using downloaded Mamba1 and Mamba2 models.

Features:
- Uses production Mamba models (130M parameters each)
- Supports both Mamba1 and Mamba2 architectures
- Interactive and automated generation modes
- Shows proper device selection and error handling
"""

import argparse
import sys
import time
from pathlib import Path

# Add src to path
sys.path.append(str(Path(__file__).parent.parent / "src"))

from mamba_macos import (generate_text_with_model, get_device,
                         load_and_prepare_model)


def demo_model(model_name, prompts, model_dir="./models"):
    """Demonstrate a specific model with various prompts."""
    print(f"\nğŸ¤– Testing {model_name.upper()}")
    print("=" * 50)
    
    device = get_device()
    print(f"ğŸ“± Device: {device}")
    
    # Load model
    success, model, tokenizer = load_and_prepare_model(model_name, model_dir, device)
    
    if not success:
        print(f"âŒ Failed to load {model_name}")
        return False
    
    # Test with different prompts
    for i, prompt_config in enumerate(prompts, 1):
        prompt = prompt_config["prompt"]
        max_length = prompt_config.get("max_length", 40)
        temperature = prompt_config.get("temperature", 0.7)
        
        print(f"\nğŸ“ Test {i}: '{prompt}'")
        print(f"âš™ï¸ Settings: max_length={max_length}, temperature={temperature}")
        
        start_time = time.time()
        try:
            generated = generate_text_with_model(
                model, tokenizer, prompt, device, max_length, temperature
            )
            gen_time = time.time() - start_time
            
            print(f"âœ… Generated ({gen_time:.2f}s):")
            print(f"   {generated}")
            
            # Calculate words per second
            words_generated = len(generated.split()) - len(prompt.split())
            if words_generated > 0 and gen_time > 0:
                wps = words_generated / gen_time
                print(f"ğŸ“Š Performance: {wps:.1f} words/sec")
                
        except Exception as e:
            print(f"âŒ Generation failed: {e}")
    
    return True


def interactive_mode():
    """Interactive mode for custom prompts."""
    print("\nğŸ¯ Interactive Mode")
    print("=" * 50)
    print("Enter prompts to test with both models (type 'quit' to exit)")
    
    device = get_device()
    models = {}
    
    # Pre-load both models
    for model_name in ["mamba1", "mamba2"]:
        print(f"\nğŸ”„ Loading {model_name}...")
        success, model, tokenizer = load_and_prepare_model(model_name, "./models", device)
        if success:
            models[model_name] = (model, tokenizer)
            print(f"âœ… {model_name} ready")
        else:
            print(f"âŒ {model_name} failed to load")
    
    if not models:
        print("âŒ No models available for interactive mode")
        return
    
    while True:
        try:
            prompt = input("\nğŸ’­ Enter your prompt: ").strip()
            
            if prompt.lower() in ['quit', 'exit', 'q']:
                print("ğŸ‘‹ Goodbye!")
                break
            
            if not prompt:
                continue
            
            # Get parameters
            try:
                max_length = int(input("ğŸ“ Max length (default 30): ") or "30")
                temperature = float(input("ğŸŒ¡ï¸ Temperature (default 0.7): ") or "0.7")
            except ValueError:
                max_length, temperature = 30, 0.7
            
            # Test with all available models
            for model_name, (model, tokenizer) in models.items():
                print(f"\nğŸ¤– {model_name.upper()}:")
                try:
                    generated = generate_text_with_model(
                        model, tokenizer, prompt, device, max_length, temperature
                    )
                    print(f"   {generated}")
                except Exception as e:
                    print(f"   âŒ Error: {e}")
                    
        except KeyboardInterrupt:
            print("\nğŸ‘‹ Interrupted by user. Goodbye!")
            break


def showcase_structure():
    """Show the organized directory structure."""
    print("\nğŸ—ï¸ Organized Directory Structure")
    print("=" * 50)
    
    structure_info = [
        ("ğŸ“¦ src/mamba_macos/", "Core library"),
        ("ğŸ”§ scripts/", "Utility scripts (download, run models)"),
        ("ğŸ§ª tests/unit/", "Component-level unit tests"),
        ("ğŸ§ª tests/integration/", "End-to-end integration tests"),
        ("ğŸ“š examples/", "Usage examples and demos"),
        ("âš™ï¸ config/", "Configuration files"),
        ("ğŸ› ï¸ tools/", "Development tools and test runners"),
        ("ğŸ¤– models/", "Downloaded model files"),
    ]
    
    for path, description in structure_info:
        print(f"{path:<25} {description}")
    
    print(f"\nğŸ¯ Key Benefits:")
    print("âœ… Clean separation of concerns")
    print("âœ… Professional Python package structure") 
    print("âœ… Easy imports: from mamba_macos import ...")
    print("âœ… Organized tests (unit + integration)")
    print("âœ… Makefile for common development tasks")


def main():
    parser = argparse.ArgumentParser(description="Mamba Production Demo")
    parser.add_argument("--interactive", action="store_true", help="Interactive mode")
    parser.add_argument("--model", choices=["mamba1", "mamba2", "both"], default="both", help="Model to test")
    parser.add_argument("--show-structure", action="store_true", help="Show directory structure")
    
    args = parser.parse_args()
    
    print("ğŸ‰ Mamba Production Demo")
    print("=" * 50)
    print("Text generation with production Mamba models")
    
    if args.show_structure:
        showcase_structure()
        return
    
    if args.interactive:
        interactive_mode()
        return
    
    # Predefined demo prompts
    demo_prompts = [
        {
            "prompt": "The future of artificial intelligence",
            "max_length": 35,
            "temperature": 0.7
        },
        {
            "prompt": "Apple Silicon processors are revolutionizing",
            "max_length": 30,
            "temperature": 0.8
        },
        {
            "prompt": "State space models represent",
            "max_length": 25,
            "temperature": 0.6
        }
    ]
    
    models_to_test = []
    if args.model in ["mamba1", "both"]:
        models_to_test.append("mamba1")
    if args.model in ["mamba2", "both"]:
        models_to_test.append("mamba2")
    
    success_count = 0
    for model_name in models_to_test:
        if demo_model(model_name, demo_prompts):
            success_count += 1
    
    print(f"\nğŸ¯ Demo Summary")
    print("=" * 50)
    print(f"âœ… Successfully tested {success_count}/{len(models_to_test)} models")
    print("ğŸ“‹ Try these commands:")
    print("  python -m examples.01_demo --interactive")
    print("  python -m examples.01_demo --show-structure")
    print("  python -m examples.01_demo --model mamba1")
    print("\nğŸ› ï¸ Development commands:")
    print("  make test-quick    # Quick integration test")
    print("  make run-mamba1    # Run Mamba1 demo")
    print("  make show-structure # Show project layout")


if __name__ == "__main__":
    main() 